{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyzaf/notebooks/blob/main/chennai_test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_psqOIQ2FUyW"
      },
      "source": [
        "# **Sentiment Analysis of Twitter Posts on Chennai Floods using Python**\n",
        "\n",
        "\n",
        "<img src=\"https://samyzaf.com/ML/chennai/Chennai_1.jpg\" width=\"75%\" />\n",
        "\n",
        "\n",
        "**Author: Yogesh Kulkarni**\n",
        "<br/>\n",
        "Source:\n",
        "https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python\n",
        "<br>\n",
        "Thanks Eran Shlomo for posting it to Linkedin\n",
        "<br/>\n",
        "Converted and annotated to a IPython notebook: Samy Zafrany\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "* The best way to learn data science is to **do data science**. No second thought about it!\n",
        "\n",
        "* One of the ways, I do this is continuously look for interesting work done by other community members. Once I understand the project, I do / improve the project on my own. Honestly, I can’t think of a better way to learn data science.\n",
        "\n",
        "* As part of my search, I came across a study on\n",
        "[sentiment analysis of Chennai Floods](https://www.analyticsvidhya.com/blog/2016/07/capstone-project/)\n",
        "on Analytics Vidhya. I decided to perform sentiment analysis of the same study using Python and add it here. Well, what can be better than building onto something great.\n",
        "\n",
        "* To get acquainted with the crisis of Chennai Floods, 2015 you can read the complete\n",
        "[study](https://www.analyticsvidhya.com/blog/2016/07/capstone-project)\n",
        "here. This study was done on a set of social interactions limited to the first two days of Chennai Floods in December 2015.\n",
        "\n",
        "* The objectives of this article is to understand the different subjects of interactions during the floods using Python. Grouping similar messages together with emphasis on predominant themes (rescue, food, supplies, ambulance calls) can help government and other authorities to act in the right manner during the crisis time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOVom_8iFUyc"
      },
      "source": [
        "## **Building Corpus**\n",
        "\n",
        "A typical tweet is mostly a text message within limit of 140 characters.\n",
        "\\#hashtags convey subject of the tweet whereas @user seeks attention of that user.\n",
        "Forwarding is denoted by ‘rt’ (retweet) and is a measure of its popularity.\n",
        "One can like a tweet by making it ‘favorite’.\n",
        "\n",
        "About 6000 twits were collected with ‘#ChennaiFloods’ hashtag and between 1st and 2nd Dec 2015.\n",
        "Jefferson’s GetOldTweets utility (got) was used in Python 2.7 to collect the older tweets.\n",
        "One can store the tweets either in a csv file or to a database like MongoDb to be used for\n",
        "further processing.\n",
        "\n",
        "The **got** Python module can be downloaded from:\n",
        "https://github.com/Jefferson-Henrique/GetOldTweets-python\n",
        "\n",
        "Note that you will also need to download Mongo from https://www.mongodb.com\n",
        "and run a mongo server on your pc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install GetOldTweets3\n",
        "%pip install pymongo"
      ],
      "metadata": {
        "id": "E2wmwtZ8TStN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import GetOldTweets3 as got\n",
        "from pymongo import MongoClient"
      ],
      "metadata": {
        "id": "8SJSXjrjUCy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = MongoClient('localhost', 27017)\n",
        "db = client['twitter_db']\n",
        "collection = db['twitter_collection']\n",
        "\n",
        "def streamTweets(tweets):\n",
        "   for t in tweets:\n",
        "      obj = {\"user\": t.username, \"retweets\": t.retweets, \"favorites\":\n",
        "            t.favorites, \"text\":t.text,\"geo\": t.geo,\"mentions\":\n",
        "            t.mentions, \"hashtags\": t.hashtags,\"id\": t.id,\n",
        "            \"permalink\": t.permalink,}\n",
        "      tweetind = collection.insert_one(obj).inserted_id"
      ],
      "metadata": {
        "id": "mks1iSZpiitq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9HLWIkgnFUyc"
      },
      "outputs": [],
      "source": [
        "tweetCriteria = (\n",
        "    got.manager.TweetCriteria()\n",
        "               .setQuerySearch('ChennaiFloods')\n",
        "               .setSince(\"2015-12-01\")\n",
        "               .setUntil(\"2015-12-02\")\n",
        "               .setMaxTweets(6000)\n",
        ")\n",
        "\n",
        "got.manager.TweetManager.getTweets(tweetCriteria, streamTweets);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-3rKHyFUyc"
      },
      "source": [
        "Tweets stored in MongoDB can be accessed from another python script.\n",
        "Following example shows how the whole db was converted to a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zg6T-MwWFUyd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "client = MongoClient('localhost', 27017)\n",
        "db = client['twitter_db']\n",
        "collection = db['twitter_collection']\n",
        "df = pd.DataFrame(list(collection.find()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL1CZLQZFUyd"
      },
      "source": [
        "Take a look at the 6 first records of this dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lNj8mesFUye"
      },
      "outputs": [],
      "source": [
        "df.head(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEV3sCtzFUye"
      },
      "source": [
        "# Data Exploration\n",
        "\n",
        "Once in dataframe format, it is easier to explore the data. Here are few examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGbCi8-cFUyf"
      },
      "outputs": [],
      "source": [
        "# FreqDist = Frequency Distribution of words list\n",
        "from nltk import FreqDist\n",
        "hashtags = []\n",
        "for hs in df[\"hashtags\"]: # Each entry may contain multiple hashtags. We need to split.\n",
        "       hashtags += hs.split(\" \")\n",
        "hashtag_dist = FreqDist(hashtags)\n",
        "hashtag_dist.plot(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oafJjbfvFUyf"
      },
      "source": [
        "As seen in the\n",
        "[study](https://www.analyticsvidhya.com/blog/2016/07/capstone-project)\n",
        "the most used tags were “#chennairains”, “#ICanAccommodate”, apart from the original query tag “#ChennaiFloods”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihL-YRpcFUyf"
      },
      "source": [
        "## Top 10 users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuSWkjgUFUyg"
      },
      "outputs": [],
      "source": [
        "users = df[\"user\"].tolist()\n",
        "users_dist = FreqDist(users)\n",
        "users_dist.plot(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAKPUvlqFUyg"
      },
      "source": [
        "# **Text Pre-processing**\n",
        "\n",
        "All tweets are processed to remove unnecessary things like links, non-English words, stopwords, punctuation’s, etc. We employ the power of the **nltk.tokenize** and **nltk.corpus** modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AffHX87KFUyg"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import re, string\n",
        "import nltk\n",
        "tweets_texts = df[\"text\"].tolist()\n",
        "stopwords = stopwords.words('english')\n",
        "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zixt7k76FUyg"
      },
      "outputs": [],
      "source": [
        "def process_tweet_text(tweet):\n",
        "   if tweet.startswith('@null'):\n",
        "       return \"[Tweet not available]\"\n",
        "   tweet = re.sub(r'\\$\\w*','',tweet) # Remove tickers\n",
        "   tweet = re.sub(r'https?:\\/\\/.*\\/\\w*','',tweet) # Remove hyperlinks\n",
        "   tweet = re.sub(r'['+string.punctuation+']+', ' ',tweet) # Remove puncutations like 's\n",
        "   twtok = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "   tokens = twtok.tokenize(tweet)\n",
        "   tokens = [i.lower() for i in tokens if i not in stopwords and len(i) > 2 and\n",
        "                                             i in english_vocab]\n",
        "   return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PLj3lBCeFUyg"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "for tw in tweets_texts:\n",
        "    words += process_tweet_text(tw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad25LR-tFUyg"
      },
      "outputs": [],
      "source": [
        "# How many words do we have?\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozEmeimQFUyh"
      },
      "outputs": [],
      "source": [
        "# Take a look at the first 15 words\n",
        "words[0:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ySK-YS0FUyh"
      },
      "source": [
        "# **Text Exploration**\n",
        "\n",
        "The words are plotted again to find the most frequently used terms. A few simple words repeat more often than others: ’help’, ‘people’, ‘stay’, ’safe’, etc.\n",
        "\n",
        "[(‘twitter’, 1026), (‘pic’, 1005), (‘help’, 569), (‘people’, 429), (‘safe’, 274)]\n",
        "\n",
        "These are immediate reactions and responses to the crisis.\n",
        "\n",
        "Some infrequent terms are [(‘fit’, 1), (‘bible’, 1), (‘disappear’, 1), (‘regulated’, 1), (‘doom’, 1)]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sRuotqCFUyh"
      },
      "outputs": [],
      "source": [
        "words_dist = FreqDist(words)\n",
        "words_dist.plot(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHS7mGlbFUyh"
      },
      "source": [
        "Collocations are the words that are found together. They can be bi-grams (two words together) or phrases like trigrams (3 words) or n-grams (n words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3EmVcfJFUyh"
      },
      "outputs": [],
      "source": [
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(words, 5)\n",
        "finder.apply_freq_filter(5)\n",
        "for word1,word2 in finder.nbest(bigram_measures.likelihood_ratio, 10):\n",
        "    print(word1, word2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nI47rpxFUyh"
      },
      "source": [
        "These depict the disastrous situation, like “stay safe”, “rescue team”, even a commonly used Hindi phrase “pani pani” (lots of water)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jObboWSFUyh"
      },
      "source": [
        "# **Clustering**\n",
        "\n",
        "In such crisis situations, lots of similar tweets are generated. They can be grouped together in clusters based on closeness or ‘distance’ amongst them. Artem Lukanin has explained the process in details here. TF-IDF method is used to vectorize the tweets and then cosine distance is measured to assess the similarity.\n",
        "\n",
        "Each tweet is pre-processed and added to a list. The list is fed to TFIDF Vectorizer to convert each tweet into a vector. Each value in the vector depends on how many times a word or a term appears in the tweet (TF) and on how rare it is amongst all tweets/documents (IDF). Below is a visual representation of TFIDF matrix it generates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zJbtSjAMFUyh"
      },
      "outputs": [],
      "source": [
        "cleaned_tweets = []\n",
        "for tw in tweets_texts:\n",
        "    words = process_tweet_text(tw)\n",
        "    #Form sentences of processed words\n",
        "    cleaned_tweet = \" \".join(w for w in words if len(w) > 2 and w.isalpha())\n",
        "    cleaned_tweets.append(cleaned_tweet)\n",
        "df['CleanTweetText'] = cleaned_tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtPsGzpRFUyi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_tweets)\n",
        "feature_names = tfidf_vectorizer.get_feature_names() # num phrases\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = 1 - cosine_similarity(tfidf_matrix)\n",
        "print(dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs1VvSBQFUyi"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 3\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "clusters = km.labels_.tolist()\n",
        "df['ClusterID'] = clusters\n",
        "print(df['ClusterID'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-KAoqrCFUyi"
      },
      "source": [
        "We obtained 3 clusers:\n",
        "1. Clustr 0 with 5498 tweets\n",
        "2. Cluster 1 with 76 tweets\n",
        "3. Cluster 2 with 426 tweets\n",
        "\n",
        "The top words used in each cluster can be computed by as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHjGsruxFUyi"
      },
      "outputs": [],
      "source": [
        "#sort cluster centers by proximity to centroid\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster {}: Words:\".format(i))\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print('    %s' % feature_names[ind])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5cVBmBfFUyi"
      },
      "source": [
        "The result is:\n",
        "\n",
        "* Cluster 0: Words: show mercy please people rain\n",
        "* Cluster 1: Words: pic twitter zoo wall broke ground saving guilty water growing\n",
        "* Cluster 2: Words: help people pic twitter safe open rain share please"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HfnfcOwFUyi"
      },
      "source": [
        "# **Doc2Vec and K-means**\n",
        "\n",
        "Doc2Vec methodology available in gensim package is used to vectorize the tweets,\n",
        "as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0JHck4XFUyi"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "taggeddocs = []\n",
        "tag2tweetmap = {}\n",
        "for index,i in enumerate(cleaned_tweets):\n",
        "    if len(i) > 2: # Non empty tweets\n",
        "        tag = u'SENT_{:d}'.format(index)\n",
        "        sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[tag])\n",
        "        tag2tweetmap[tag] = i\n",
        "        taggeddocs.append(sentence)\n",
        "model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)\n",
        "for epoch in range(60):\n",
        "    if epoch % 20 == 0:\n",
        "        print('Training epoch: %s' % epoch)\n",
        "    model.train(taggeddocs)\n",
        "    model.alpha -= 0.002  # decrease the learning rate\n",
        "    model.min_alpha = model.alpha  # fix the learning rate, no decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ3Ip16DFUyj"
      },
      "source": [
        "Once trained model is ready the tweet-vectors available in model can be clustered using K-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th3lIGdOFUyj"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "dataSet = model.syn0\n",
        "kmeansClustering = KMeans(n_clusters=6)\n",
        "centroidIndx = kmeansClustering.fit_predict(dataSet)\n",
        "topic2wordsmap = {}\n",
        "for i,val in enumerate(dataSet):\n",
        "    tag = model.docvecs.index_to_doctag(i)\n",
        "    topic = centroidIndx[i]\n",
        "    if topic in topic2wordsmap.keys():\n",
        "        for w in (tag2tweetmap[tag].split()):\n",
        "            topic2wordsmap[topic].append(w)\n",
        "    else:\n",
        "        topic2wordsmap[topic] = []\n",
        "for i in topic2wordsmap:\n",
        "    words = topic2wordsmap[i]\n",
        "    print(\"Topic {} has words {}\".format(i, words[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuw23zf_FUyj"
      },
      "outputs": [],
      "source": [
        "dataSet.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me-JoOvyFUyj"
      },
      "source": [
        "# **End Notes**\n",
        "\n",
        "This article shows how to implement Capstone-Chennai Floods study using Python and its libraries.\n",
        "With this tutorial, one can get introduction to various Natural Language Processing (NLP)\n",
        "workflows such as accessing twitter data, pre-processing text, explorations, clustering\n",
        "and topic modeling."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}